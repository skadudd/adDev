{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이스라인 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_device :  mps\n"
     ]
    }
   ],
   "source": [
    "is_mps = torch.backends.mps.is_available()\n",
    "device = torch.device('mps' if is_mps else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('current_device : ',device)\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.mkdir('/Volumes/My Passport/dataset/data')\n",
    "# !unzip -qq '/Volumes/My Passport/dataset/dataset.zip' -d '/Volumes/My Passport/dataset/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#폴더 이미 생성되어 주석처리\n",
    "original_dataset_dir = '/Volumes/My Passport/dataset/data'\n",
    "classes_list = []\n",
    "\n",
    "for i in os.listdir(original_dataset_dir):\n",
    "    if os.path.isdir(os.path.join(original_dataset_dir,i)):\n",
    "        classes_list.append(i)\n",
    "\n",
    "base_dir = '/Volumes/My Passport/dataset/splitted'\n",
    "# os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pepper,_bell___healthy\n",
      "Grape___Esca_(Black_Measles)\n",
      "Pepper,_bell___Bacterial_spot\n",
      "Strawberry___healthy\n",
      "Grape___Black_rot\n",
      "Corn___Common_rust\n",
      "Apple___Apple_scab\n",
      "Potato___healthy\n",
      "Potato___Late_blight\n",
      "Cherry___healthy\n",
      "Tomato___Bacterial_spot\n",
      "Apple___Black_rot\n",
      "Cherry___Powdery_mildew\n",
      "Corn___Cercospora_leaf_spot Gray_leaf_spot\n",
      "Peach___healthy\n",
      "Tomato___Early_blight\n",
      "Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "Potato___Early_blight\n",
      "Grape___healthy\n",
      "Apple___Cedar_apple_rust\n",
      "Corn___Northern_Leaf_Blight\n",
      "Tomato___Septoria_leaf_spot\n",
      "Corn___healthy\n",
      "Strawberry___Leaf_scorch\n",
      "Tomato___Tomato_mosaic_virus\n",
      "Tomato___Leaf_Mold\n",
      "Tomato___Late_blight\n",
      "Tomato___healthy\n",
      "Tomato___Spider_mites Two-spotted_spider_mite\n",
      "Apple___healthy\n",
      "Tomato___Target_Spot\n",
      "Peach___Bacterial_spot\n"
     ]
    }
   ],
   "source": [
    "#데이터 분기할 폴더 트리 생성\n",
    "import shutil\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "# os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "# os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "# os.mkdir(test_dir)\n",
    "\n",
    "for cls in classes_list:\n",
    "    print(cls)\n",
    "    # os.mkdir(os.path.join(train_dir,cls))\n",
    "    # os.mkdir(os.path.join(validation_dir,cls))\n",
    "    # os.mkdir(os.path.join(test_dir,cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 현황 확인\n",
    "#신규 트리구조로 파일 복사\n",
    "import math\n",
    "\n",
    "for cls in classes_list:\n",
    "    if os.path.isdir(os.path.join(original_dataset_dir, cls)):\n",
    "        path = os.path.join(original_dataset_dir,cls)\n",
    "        #DS.store 파일 무시\n",
    "        fnames = sorted((f for f in os.listdir(path) if not f.startswith('.')),key=str.lower)\n",
    "        print(len(fnames))\n",
    "        train_size = math.floor(len(fnames) * 0.6)\n",
    "        validation_size = math.floor(len(fnames) * 0.2)\n",
    "        test_size = math.floor(len(fnames) * 0.2)\n",
    "        \n",
    "        train_fnames = fnames[:train_size]\n",
    "        print(\"Train size(\",cls,\") \", len(train_fnames))\n",
    "        for fname in train_fnames:\n",
    "            source = os.path.join(path,fname)\n",
    "            destination = os.path.join(os.path.join(train_dir,cls),fname)\n",
    "            shutil.copyfile(source,destination)\n",
    "        \n",
    "        validation_fnames = fnames[train_size:(train_size+validation_size)]\n",
    "        print(\"Validation size(\",cls,\") \", len(validation_fnames))\n",
    "        for fname in validation_fnames:\n",
    "            source = os.path.join(path,fname)\n",
    "            destination = os.path.join(os.path.join(validation_dir,cls),fname)\n",
    "            shutil.copyfile(source,destination)\n",
    "        \n",
    "        test_fnames = fnames[(train_size+validation_size):(validation_size+train_size+test_size)]\n",
    "        print(\"Test size(\",cls,\") \", len(test_fnames))\n",
    "        for fname in test_fnames:\n",
    "            source = os.path.join(path,fname)\n",
    "            destination = os.path.join(os.path.join(test_dir,cls),fname)\n",
    "            shutil.copyfile(source,destination)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dsfile 삭제\n",
    "folders = [train_dir,validation_dir,test_dir]\n",
    "\n",
    "for folder in folders:\n",
    "    for cls in classes_list:\n",
    "        target = folder+'/'+cls\n",
    "        for root, dirs, files in os.walk(target):\n",
    "            for file in files:\n",
    "                if file.startswith('.'):\n",
    "                    os.remove(os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "#이미지 사이즈 조정 및 텐서화\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])\n",
    "#이미지폴더 : 폴더를 클래스화. 폴더 이름을 라벨로 지정\n",
    "train_dataset = ImageFolder(root=train_dir,transform=transform_base)\n",
    "validation_dataset = ImageFolder(root=validation_dir,transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 4\n",
    "                          )\n",
    "val_loader = DataLoader(validation_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 4\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,32,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(64,64,3,padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096,512)\n",
    "        self.fc2 = nn.Linear(512,33)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x,p=0.25,training=self.training)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x,p=0.25,training=self.training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x,p=0.25,training=self.training)\n",
    "\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = Net().to(device)\n",
    "optimizer = optim.Adam(model_base.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data,target = data.to(device),target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output,target,reduction='sum').sum()\n",
    "            pred = output.max(1,keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct/len(test_loader.dataset)\n",
    "    return test_loss,test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
    "    best_acc = 0.0\n",
    "    best_model_weight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader,optimizer)\n",
    "        train_loss,train_acc = evaluate(model,train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_weight = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('------------- epoch {} -------------'.format(epoch))\n",
    "        print('train loss:{:.4f}, Accuracy:{:.2f}%'.format(train_loss,train_acc))\n",
    "        print('val loss:{:.4f}, Accuracy:{:.2f}%'.format(val_loss,val_acc))\n",
    "        print('completed {:.0f}m {:.0f}s'.format(time_elapsed//60,time_elapsed%60))\n",
    "    \n",
    "    model.load_state_dict(best_model_weight)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 위해 모델을 저장할 때는 그 모델의 학습된 매개변수만 저장하면 됩니다. torch.save() 를 사용하여 모델의 state_dict 를 저장하는 것이 나중에 모델을 사용할 때 가장 유연하게 사용할 수 있는, 모델 저장 시 권장하는 방법입니다.\n",
    "\n",
    "PyTorch에서는 모델을 저장할 때 .pt 또는 .pth 확장자를 사용하는 것이 일반적인 규칙입니다.\n",
    "\n",
    "추론을 실행하기 전에 반드시 model.eval() 을 호출하여 드롭아웃 및 배치 정규화를 평가 모드로 설정하여야 합니다. 이 과정을 거치지 않으면 일관성 없는 추론 결과가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- epoch 1 -------------\n",
      "train loss:0.8813, Accuracy:74.68%\n",
      "val loss:0.9231, Accuracy:73.34%\n",
      "completed 2m 20s\n",
      "------------- epoch 2 -------------\n",
      "train loss:0.6885, Accuracy:77.99%\n",
      "val loss:0.7428, Accuracy:76.76%\n",
      "completed 2m 18s\n",
      "------------- epoch 3 -------------\n",
      "train loss:0.4274, Accuracy:87.40%\n",
      "val loss:0.4882, Accuracy:85.12%\n",
      "completed 2m 19s\n",
      "------------- epoch 4 -------------\n",
      "train loss:0.3710, Accuracy:88.02%\n",
      "val loss:0.4549, Accuracy:85.35%\n",
      "completed 2m 18s\n",
      "------------- epoch 5 -------------\n",
      "train loss:0.3495, Accuracy:88.63%\n",
      "val loss:0.4453, Accuracy:85.22%\n",
      "completed 2m 17s\n",
      "------------- epoch 6 -------------\n",
      "train loss:0.2308, Accuracy:93.18%\n",
      "val loss:0.3342, Accuracy:89.40%\n",
      "completed 18m 53s\n",
      "------------- epoch 7 -------------\n",
      "train loss:0.2068, Accuracy:93.84%\n",
      "val loss:0.3243, Accuracy:89.47%\n",
      "completed 10m 4s\n",
      "------------- epoch 8 -------------\n",
      "train loss:0.1604, Accuracy:95.11%\n",
      "val loss:0.2802, Accuracy:90.93%\n",
      "completed 21m 9s\n",
      "------------- epoch 9 -------------\n",
      "train loss:0.1497, Accuracy:95.57%\n",
      "val loss:0.2756, Accuracy:91.50%\n",
      "completed 2m 17s\n",
      "------------- epoch 10 -------------\n",
      "train loss:0.1373, Accuracy:95.78%\n",
      "val loss:0.2688, Accuracy:91.09%\n",
      "completed 2m 17s\n",
      "------------- epoch 11 -------------\n",
      "train loss:0.0999, Accuracy:97.07%\n",
      "val loss:0.2335, Accuracy:92.41%\n",
      "completed 2m 18s\n",
      "------------- epoch 12 -------------\n",
      "train loss:0.0970, Accuracy:97.40%\n",
      "val loss:0.2255, Accuracy:92.70%\n",
      "completed 2m 18s\n",
      "------------- epoch 13 -------------\n",
      "train loss:0.0928, Accuracy:97.17%\n",
      "val loss:0.2411, Accuracy:92.36%\n",
      "completed 2m 19s\n",
      "------------- epoch 14 -------------\n",
      "train loss:0.1116, Accuracy:96.46%\n",
      "val loss:0.2661, Accuracy:91.55%\n",
      "completed 2m 19s\n",
      "------------- epoch 15 -------------\n",
      "train loss:0.0820, Accuracy:97.47%\n",
      "val loss:0.2291, Accuracy:92.78%\n",
      "completed 2m 18s\n",
      "------------- epoch 16 -------------\n",
      "train loss:0.0696, Accuracy:98.06%\n",
      "val loss:0.2140, Accuracy:93.45%\n",
      "completed 2m 18s\n",
      "------------- epoch 17 -------------\n",
      "train loss:0.0922, Accuracy:97.31%\n",
      "val loss:0.2509, Accuracy:92.08%\n",
      "completed 2m 17s\n",
      "------------- epoch 18 -------------\n",
      "train loss:0.1026, Accuracy:96.71%\n",
      "val loss:0.2702, Accuracy:91.68%\n",
      "completed 2m 17s\n",
      "------------- epoch 19 -------------\n",
      "train loss:0.0675, Accuracy:97.86%\n",
      "val loss:0.2352, Accuracy:92.60%\n",
      "completed 2m 17s\n",
      "------------- epoch 20 -------------\n",
      "train loss:0.0594, Accuracy:98.16%\n",
      "val loss:0.2176, Accuracy:93.30%\n",
      "completed 2m 18s\n",
      "------------- epoch 21 -------------\n",
      "train loss:0.0452, Accuracy:98.94%\n",
      "val loss:0.1845, Accuracy:94.00%\n",
      "completed 2m 18s\n",
      "------------- epoch 22 -------------\n",
      "train loss:0.1037, Accuracy:96.50%\n",
      "val loss:0.2984, Accuracy:90.89%\n",
      "completed 2m 18s\n",
      "------------- epoch 23 -------------\n",
      "train loss:0.0674, Accuracy:97.67%\n",
      "val loss:0.2564, Accuracy:92.56%\n",
      "completed 2m 19s\n",
      "------------- epoch 24 -------------\n",
      "train loss:0.0541, Accuracy:98.44%\n",
      "val loss:0.2135, Accuracy:93.40%\n",
      "completed 2m 18s\n",
      "------------- epoch 25 -------------\n",
      "train loss:0.0439, Accuracy:98.67%\n",
      "val loss:0.2176, Accuracy:93.67%\n",
      "completed 2m 18s\n",
      "------------- epoch 26 -------------\n",
      "train loss:0.0417, Accuracy:98.79%\n",
      "val loss:0.2018, Accuracy:93.77%\n",
      "completed 2m 17s\n",
      "------------- epoch 27 -------------\n",
      "train loss:0.0485, Accuracy:98.51%\n",
      "val loss:0.2230, Accuracy:93.24%\n",
      "completed 2m 17s\n",
      "------------- epoch 28 -------------\n",
      "train loss:0.0441, Accuracy:98.61%\n",
      "val loss:0.2283, Accuracy:93.07%\n",
      "completed 2m 17s\n",
      "------------- epoch 29 -------------\n",
      "train loss:0.0206, Accuracy:99.60%\n",
      "val loss:0.1726, Accuracy:94.93%\n",
      "completed 2m 17s\n",
      "------------- epoch 30 -------------\n",
      "train loss:0.0345, Accuracy:99.00%\n",
      "val loss:0.1993, Accuracy:93.90%\n",
      "completed 2m 17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = train_baseline(model_base,train_loader,val_loader,optimizer,epochs)\n",
    "torch.save(base,'badseline.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전이학습 모델"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "data_transforms = {\n",
    "    #Compose : 전처리할때 사용됨.\n",
    "    'train' : \n",
    "        transforms.Compose([transforms.Resize([64,64]),\n",
    "        #기하학적 증강. 과적합을 방지하기 위함\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        #RGB 색상값의 평균,표준편차. 색상분포의 히스토그램을 찾아 적용하는 것.\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]) ]),\n",
    "    'validation' : \n",
    "        transforms.Compose([transforms.Resize([64,64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리된 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_datasets = {x: ImageFolder(root=os.path.join(base_dir,x),\n",
    "                                 transform=data_transforms[x]) for x in ['train','validation']}\n",
    "dataloader = {x : DataLoader(image_datasets[x],\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=4,\n",
    "                             ) for x in ['train','validation']}\n",
    "dataset_sizes = {x : len(image_datasets[x]) for x in ['train','validation']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet50 모델 fineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nam-yeong/miniforge3/envs/torch_nightly/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nam-yeong/miniforge3/envs/torch_nightly/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "#resnet50 모델 가져온다 (pretrained된 모델)\n",
    "resnet = models.resnet50(pretrained = True)\n",
    "#기성모델의 마지막 fully connected layer 채널 수를 우리의 입력값으로 변경해줘야함. baseline model = 33\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs,33)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#마지막 fc가 변경되었으므로 학습해야 할 필요 있음. filter는 첫번째 인자가 True인 값만 걸러내는 기능.\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad,resnet.parameters()),lr=0.001)\n",
    "\n",
    "#초기에 lr(learning-rate)를 크게주어 학습 속도를 높이고 이후 세밀조정 하는 기능\n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_shceduler = lr_scheduler.StepLR(optimizer_ft,step_size=7,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50에는 10개의 레이어가 있음. 입력에 가까운 0-5번 레이어까지 freeze\n",
    "ct = 0\n",
    "for child in resnet.children():\n",
    "    ct += 1\n",
    "    if ct < 6:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "def train_resnet(model,criterion,optimizer,scheduler,num_epochs=25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------------epoch {}-------------------'.format(epoch+1))\n",
    "        since = time.time()\n",
    "        for phase in ['train','validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "            for inputs,labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_ 는 최댓값, preds는 최댓값의 위치 인덱스\n",
    "                    _, preds = torch.max(outputs,1) #torch.max(텐서, 찾으려는 최댓값을 가진 차원(0 = 열, 1 = 행)). \n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0) #미니배치에서의 손실이니, 미니배치의 크기를 곱하여 전체 손실의 계산\n",
    "                running_correct += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss/dataset_sizes[phase]\n",
    "            # epoch_acc = running_correct.double()/dataset_sizes[phase] #정확도 계산에 필요한 타입 변환(Type Casting)을 수행 double()\n",
    "            epoch_acc = running_correct.float()/dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss : {:.4f} Acc : {:.4f}'.format(phase,epoch_loss,epoch_acc))\n",
    "\n",
    "            if phase=='validation'and epoch_acc>best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60,time_elapsed % 60))\n",
    "    print('Best val ACC : {:.2f}',format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------epoch 1-------------------\n",
      "train Loss : 0.3733 Acc : 0.8821\n",
      "validation Loss : 0.2537 Acc : 0.9149\n",
      "Completed in 3m 5s\n",
      "-------------------epoch 2-------------------\n",
      "train Loss : 0.2701 Acc : 0.9104\n",
      "validation Loss : 0.2125 Acc : 0.9254\n",
      "Completed in 3m 19s\n",
      "-------------------epoch 3-------------------\n",
      "train Loss : 0.2312 Acc : 0.9243\n",
      "validation Loss : 0.1418 Acc : 0.9538\n",
      "Completed in 3m 31s\n",
      "-------------------epoch 4-------------------\n",
      "train Loss : 0.2010 Acc : 0.9346\n",
      "validation Loss : 0.1211 Acc : 0.9602\n",
      "Completed in 3m 30s\n",
      "-------------------epoch 5-------------------\n",
      "train Loss : 0.2045 Acc : 0.9355\n",
      "validation Loss : 0.1493 Acc : 0.9498\n",
      "Completed in 3m 47s\n",
      "-------------------epoch 6-------------------\n",
      "train Loss : 0.1679 Acc : 0.9437\n",
      "validation Loss : 0.1555 Acc : 0.9473\n",
      "Completed in 3m 26s\n",
      "-------------------epoch 7-------------------\n",
      "train Loss : 0.1022 Acc : 0.9664\n",
      "validation Loss : 0.0658 Acc : 0.9775\n",
      "Completed in 3m 27s\n",
      "-------------------epoch 8-------------------\n",
      "train Loss : 0.0682 Acc : 0.9768\n",
      "validation Loss : 0.0507 Acc : 0.9824\n",
      "Completed in 3m 25s\n",
      "-------------------epoch 9-------------------\n",
      "train Loss : 0.0617 Acc : 0.9792\n",
      "validation Loss : 0.0479 Acc : 0.9827\n",
      "Completed in 3m 20s\n",
      "-------------------epoch 10-------------------\n",
      "train Loss : 0.0540 Acc : 0.9822\n",
      "validation Loss : 0.0471 Acc : 0.9844\n",
      "Completed in 3m 21s\n",
      "-------------------epoch 11-------------------\n",
      "train Loss : 0.0509 Acc : 0.9829\n",
      "validation Loss : 0.0456 Acc : 0.9851\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 12-------------------\n",
      "train Loss : 0.0483 Acc : 0.9834\n",
      "validation Loss : 0.0397 Acc : 0.9862\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 13-------------------\n",
      "train Loss : 0.0436 Acc : 0.9842\n",
      "validation Loss : 0.0409 Acc : 0.9857\n",
      "Completed in 3m 23s\n",
      "-------------------epoch 14-------------------\n",
      "train Loss : 0.0404 Acc : 0.9858\n",
      "validation Loss : 0.0353 Acc : 0.9872\n",
      "Completed in 3m 20s\n",
      "-------------------epoch 15-------------------\n",
      "train Loss : 0.0352 Acc : 0.9880\n",
      "validation Loss : 0.0344 Acc : 0.9882\n",
      "Completed in 3m 25s\n",
      "-------------------epoch 16-------------------\n",
      "train Loss : 0.0340 Acc : 0.9887\n",
      "validation Loss : 0.0343 Acc : 0.9879\n",
      "Completed in 3m 25s\n",
      "-------------------epoch 17-------------------\n",
      "train Loss : 0.0350 Acc : 0.9879\n",
      "validation Loss : 0.0313 Acc : 0.9896\n",
      "Completed in 3m 23s\n",
      "-------------------epoch 18-------------------\n",
      "train Loss : 0.0350 Acc : 0.9884\n",
      "validation Loss : 0.0342 Acc : 0.9869\n",
      "Completed in 3m 21s\n",
      "-------------------epoch 19-------------------\n",
      "train Loss : 0.0351 Acc : 0.9882\n",
      "validation Loss : 0.0324 Acc : 0.9905\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 20-------------------\n",
      "train Loss : 0.0321 Acc : 0.9890\n",
      "validation Loss : 0.0312 Acc : 0.9891\n",
      "Completed in 3m 22s\n",
      "-------------------epoch 21-------------------\n",
      "train Loss : 0.0309 Acc : 0.9894\n",
      "validation Loss : 0.0327 Acc : 0.9887\n",
      "Completed in 3m 26s\n",
      "-------------------epoch 22-------------------\n",
      "train Loss : 0.0315 Acc : 0.9891\n",
      "validation Loss : 0.0320 Acc : 0.9884\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 23-------------------\n",
      "train Loss : 0.0325 Acc : 0.9894\n",
      "validation Loss : 0.0321 Acc : 0.9890\n",
      "Completed in 3m 28s\n",
      "-------------------epoch 24-------------------\n",
      "train Loss : 0.0318 Acc : 0.9895\n",
      "validation Loss : 0.0328 Acc : 0.9887\n",
      "Completed in 3m 29s\n",
      "-------------------epoch 25-------------------\n",
      "train Loss : 0.0318 Acc : 0.9887\n",
      "validation Loss : 0.0375 Acc : 0.9870\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 26-------------------\n",
      "train Loss : 0.0308 Acc : 0.9902\n",
      "validation Loss : 0.0320 Acc : 0.9886\n",
      "Completed in 3m 27s\n",
      "-------------------epoch 27-------------------\n",
      "train Loss : 0.0330 Acc : 0.9888\n",
      "validation Loss : 0.0314 Acc : 0.9904\n",
      "Completed in 3m 24s\n",
      "-------------------epoch 28-------------------\n",
      "train Loss : 0.0306 Acc : 0.9896\n",
      "validation Loss : 0.0288 Acc : 0.9896\n",
      "Completed in 3m 31s\n",
      "-------------------epoch 29-------------------\n",
      "train Loss : 0.0329 Acc : 0.9885\n",
      "validation Loss : 0.0307 Acc : 0.9896\n",
      "Completed in 3m 29s\n",
      "-------------------epoch 30-------------------\n",
      "train Loss : 0.0303 Acc : 0.9893\n",
      "validation Loss : 0.0329 Acc : 0.9889\n",
      "Completed in 3m 23s\n",
      "Best val ACC : {:.2f} 0.9904869198799133\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet,criterion,optimizer_ft,exp_lr_shceduler,num_epochs=epochs)\n",
    "torch.save(model_resnet50,'resnet50.pt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 로드\n",
    "transform_resNet = transforms.Compose([\n",
    "    transforms.Resize([64,64]),\n",
    "    transforms.RandomCrop(52),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.586,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_resNet = ImageFolder(root=os.path.join(test_dir),transform=transform_resNet)\n",
    "test_loader_resNet = DataLoader(test_resNet,batch_size=batch_size,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet test acc:  59.99499311553386\n"
     ]
    }
   ],
   "source": [
    "#모델 로드\n",
    "resnet50 = torch.load('resnet50.pt')\n",
    "resnet50.eval()\n",
    "test_loss,test_accuracy = evaluate(resnet50,test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
